import re
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Rule-based cleanup
def clean_text(prompt: str) -> str:
    fillers = [
        r"\bplease\b", r"\bwrite me\b", r"\bi need\b", r"\bcan you\b",
        r"\bcould you\b", r"\bcreate\b", r"\bgenerate\b", r"\bshow me\b",
        r"\breturn\b", r"\bthat\b", r"\bwhich\b"
    ]
    for filler in fillers:
        prompt = re.sub(filler, "", prompt, flags=re.IGNORECASE)
    return re.sub(r'\s+', ' ', prompt).strip()


# Keyword breakdown
def to_keyword_style(prompt: str) -> str:
    parts = re.split(r'[.;]', prompt)
    parts = [p.strip() for p in parts if p.strip()]
    return "\n".join(f"- {p}" for p in parts)


# LLM-powered refinement
class PromptOptimizer:
    def __init__(self, model_name="google/flan-t5-small"):
        print("Loading model... (one-time load)")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    def llm_optimize(self, prompt: str) -> str:
        input_text = f"Rewrite concisely as structured instructions:\n{prompt}"
        inputs = self.tokenizer(input_text, return_tensors="pt", truncation=True)
        output = self.model.generate(**inputs, max_length=100)
        return self.tokenizer.decode(output[0], skip_special_tokens=True)


def optimize_prompt(raw_prompt: str, use_llm=True) -> str:
    cleaned = clean_text(raw_prompt)
    keyword_form = to_keyword_style(cleaned)

    if use_llm:
        optimizer = PromptOptimizer()
        llm_refined = optimizer.llm_optimize(keyword_form)
        return f"ðŸ§  LLM-Optimized Prompt:\n\n{llm_refined}\n"
    else:
        return f"ðŸ§ª Rule-based Optimized Prompt:\n\n{keyword_form}\n"


if __name__ == "__main__":
    raw = input("Enter your prompt:\n")
    print("\n" + "=" * 40)
    print(optimize_prompt(raw_prompt=raw))
    print("=" * 40)
